import itertools
import string
import numpy as np

localrules: aggregate_vep, aggregate_catalytic, aggregate_features

configfile: "config/config.yml"

RESULTS_DIR = config["results_dir"]
DATA_DIR = config["data_dir"]
LOG_DIR = config["log_dir"]
SCRIPTS_DIR = config["scripts_dir"]
UNIPROT_IDMAPPING = config["uniprot_idmapping"]
ALPHAFOLD_DIR = config["alphafold_dir"]
MANE_DIR = config["mane_dir"]
DSSP_ALPHAFOLD = config["dssp_alphafold"]
DSSP_MANE = config["dssp_mane"]
PANTHER = config["panther"]
PDBMODELS = config["pdbmodels"]
DSSPS = config["dssps"]

chr_format = config["chr_format"]

# Make the chromosome strings according to the format in the VCF
if chr_format == 'string':
    chromosomes = ["chr" + str(i) for i in range(1, 23)] + ["chrX", "chrY"]
else:
    chromosomes = [str(i) for i in range(1,23)] + ["X", "Y"]
    
MAX_VARIANTS = config["max_variants_per_vep_run"]
MAX_JOBS = config["max_jobs"]

wildcard_constraints:
    dataset="|".join(config["datasets"]),
    chromosome="|".join(chromosomes),
    i="[a-z][a-z]",
    j="\d+-\d+"


rule targets:
    input:
        expand("{RESULTS_DIR}/{dataset}_annotations.pkl",
                RESULTS_DIR=RESULTS_DIR,
                dataset=config["datasets"])


rule pysam_compress:
    """
    Compress the VCF file using pysam
    """
    input:
        lambda wildcards: config["datasets"][wildcards.dataset]
    output:
        RESULTS_DIR + "/{dataset}.vcf.bgz"
    threads: 1
    resources:
        runtime=30,
        mem_mb=4000,
        nodes=1
    container:
        "docker://gzmf/pysam_compress:0.1"
    log:
        LOG_DIR + "/01_pysam_compress/{dataset}.log"
    shell:
        """
        python {SCRIPTS_DIR}/pysam_compress.py {input} {output} > {log} 2>&1
        """


rule tabix_index:
    """
    Index the VCF file using tabix
    """
    input:
        RESULTS_DIR + "/{dataset}.vcf.bgz"
    output:
        RESULTS_DIR + "/{dataset}.vcf.bgz.tbi"
    threads: 1
    resources:
        runtime=30,
        mem_mb=4000,
        nodes=1
    container:
        "docker://gzmf/tabix_index:0.1"
    log:
        LOG_DIR + "/02_tabix_index/{dataset}.log"
    shell:
        """
        tabix {input} > {log} 2>&1
        """


rule split_chromosomes:
    """
    Split the VCF file by chromosome
    """
    input:
        vcf=RESULTS_DIR + "/{dataset}.vcf.bgz",
        tbi=RESULTS_DIR + "/{dataset}.vcf.bgz.tbi"
    output:
        RESULTS_DIR + "/{dataset}_chrs/{chromosome}.vcf"
    threads: 1
    resources:
        runtime=30,
        mem_mb=4000,
        nodes=1
    params:
        out_dir=RESULTS_DIR + "/{dataset}_chrs"
    container:
        "docker://staphb/bcftools:1.19"
    log:
        LOG_DIR + "/03_split_chromosomes/{dataset}_{chromosome}.log"
    shell:
        """
        mkdir -p {params.out_dir}
        bcftools view {input.vcf} --regions {wildcards.chromosome} > \
        {params.out_dir}/{wildcards.chromosome}.vcf 2> {log}
        """


rule export_variants:
    """
    Create a table with the variants in the VCF file, to be able to count them
    """
    input:
        vcf=RESULTS_DIR + "/{dataset}_chrs/{chromosome}.vcf"
    output:
        RESULTS_DIR + "/{dataset}_chrs/{chromosome}_variants.txt"
    threads: 1
    resources:
        runtime=30,
        mem_mb=4000,
        nodes=1
    container:
        "docker://staphb/bcftools:1.19"
    log:
        LOG_DIR + "/04_variants_table/{dataset}_{chromosome}.log"
    shell:
        """
        bcftools query -f'%CHROM\t%POS\n' {input.vcf} > {output} 2> {log}
        """


checkpoint split_variants:
    """
    Split the chromosome files into smaller files with a maximum number of variants
    Define this rule as a checkpoint because it generates multiple output files
    """
    input:
        RESULTS_DIR + "/{dataset}_chrs/{chromosome}_variants.txt"
    output:
        split_dir=directory(RESULTS_DIR + "/{dataset}_chrs/{chromosome}_split")
    threads: 1
    resources:
        runtime=30,
        mem_mb=2000,
        nodes=1
    params:
        max_variants=MAX_VARIANTS,
        output_dir=RESULTS_DIR + "/{dataset}_chrs/{chromosome}_split"
    shell:
        """
        mkdir {params.output_dir}; \
        cat {input} | split -l {params.max_variants} - {params.output_dir}/chunk_
        """


rule extract_variants:
    """
    Extract the variants listed in the chunk file
    """
    input:
        vcf=RESULTS_DIR + "/{dataset}_chrs/{chromosome}.vcf",
        chunk=RESULTS_DIR + "/{dataset}_chrs/{chromosome}_split/chunk_{i}"
    output:
        RESULTS_DIR + "/{dataset}_chrs/{chromosome}_split/chunk_{i}.vcf"
    threads: 1
    resources:
        runtime=30,
        mem_mb=4000,
        nodes=1
    container:
        "docker://staphb/bcftools:1.19"
    log:
        LOG_DIR + "/05_extract_variants/{dataset}_{chromosome}_{i}.log"
    shell:
        """
        bcftools view -T {input.chunk} {input.vcf} > {output} 2> {log}
        """

rule run_vep:
    """
    Run the VEP on the extracted variants
    """
    input:
        RESULTS_DIR + "/{dataset}_chrs/{chromosome}_split/chunk_{i}.vcf"
    output:
        RESULTS_DIR + "/{dataset}_chrs/{chromosome}_split/chunk_{i}.vcf.vep"
    threads: 4
    resources:
        runtime=60,
        mem_mb=8000,
        nodes=1
    container:
        "docker://ensemblorg/ensembl-vep:release_111.0"
    log:
        LOG_DIR + "/06_run_vep/{dataset}_{chromosome}_{i}.log"
    shell:
        """
        vep \
        --dir /data \
        -i {input} \
        -o {output} \
        --assembly GRCh38 \
        --species homo_sapiens \
        --offline --cache --cache_version 111 \
        --fork {threads} --format vcf --total_length --vcf \
        --uniprot \
        --mane \
        --mane_select \
        --protein \
        --canonical \
        --biotype \
        --domains \
        --xref_refseq \
        --check_existing \
        --variant_class \
        --sift b \
        --polyphen b \
        --force_overwrite \
        --no_stats > {log} 2>&1
        """


def aggregate_input(wildcards):
    checkpoint_output = checkpoints.split_variants.get(**wildcards).output[0]
    return expand("{RESULTS_DIR}/{dataset}_chrs/{chromosome}_split/chunk_{i}.vcf.vep",
                    RESULTS_DIR=RESULTS_DIR,
                    dataset=wildcards.dataset,
                    chromosome=wildcards.chromosome,
                    i=glob_wildcards(os.path.join(checkpoint_output, "chunk_{i,[a-z][a-z]}")).i)


rule aggregate_vep:
    """
    Aggregate rule to check that VEP was run correctly on all chunks.
    It has to be a separate rule to read_vcfs because the input is a function
    that needs the chromosome and dataset wildcards.
    """
    input:
        aggregate_input
    params:
        chrs_dir=RESULTS_DIR + "/{dataset}_chrs"
    threads: 1
    output:
        RESULTS_DIR + "/{dataset}_chrs/{chromosome}_vep_done"
    shell:
        "echo {input} > {output}"


rule read_process_filter_vcfs:
    """
    Read all vcfs and merge them into a single pandas DataFrame
    """
    input:
        expand("{RESULTS_DIR}/{dataset}_chrs/{chromosome}_vep_done",
                RESULTS_DIR=RESULTS_DIR,
                dataset=config["datasets"],
                chromosome=chromosomes)
    params:
        chrs_dir=RESULTS_DIR + "/{dataset}_chrs",
        infos=SCRIPTS_DIR + "/info_fields.txt"
    output:
        RESULTS_DIR + "/{dataset}_annotations/05_variants.csv"
    threads: 1
    resources:
        runtime=120,
        mem_mb=400000,
        nodes=1
    container:
        "docker://gzmf/read_vcfs:0.1"
    log:
        LOG_DIR + "/07_read_process_filter_vcfs/{dataset}.log"
    shell:
        """
        python {SCRIPTS_DIR}/process_filter_variants.py {params.chrs_dir} {params.infos} \
        {output} > {log} 2>&1
        """


checkpoint get_uniprot_ids:
    """
    Obtain the UniProt IDs for the variants. It also creates a directory with
    the variants split in multiple DataFrames.

    It is a checkpoint because it generates multiple output files.
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/05_variants.csv"
    output:
        out_ranges=directory(RESULTS_DIR + "/{dataset}_annotations/07_ranges")
    threads: 40
    resources:
        runtime=180,
        mem_mb=400000,
        nodes=1
    params:
        chunk_size=100000
    container:
        "docker://gzmf/read_vcfs:0.1"
    log:
        LOG_DIR + "/12_idmapping/{dataset}.log"
    shell:
        """
        python {SCRIPTS_DIR}/get_uniprot_ids.py {input} {UNIPROT_IDMAPPING} \
        {output.out_ranges} --cpus {threads} \
        --chunk_size {params.chunk_size} --max_jobs {MAX_JOBS} > {log} 2>&1
        """


rule find_pdbs:
    """
    Find the PDBs for every variant
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/07_ranges/07_variants_{j}.pkl",
    output:
        RESULTS_DIR + "/{dataset}_annotations/08_variants/11_variants_{j}.pkl"
    threads: 8
    resources:
        mem_mb=lambda wc, input: max(10 * input.size_mb, 2000),
        runtime=120,
        nodes=1
    container:
        "docker://gzmf/calculate_features:0.1"
    log:
        LOG_DIR + "/13_find_pdbs/{dataset}_{j}.log"
    shell:
        """
        python {SCRIPTS_DIR}/find_pdbs_optimized.py {input} \
        {ALPHAFOLD_DIR} {MANE_DIR} {output} --cpus {threads} > {log} 2>&1
        """


rule get_catalytic:
    """
    Find the catalytic residues for the UniProt IDs in every variant chunk file
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/08_variants/11_variants_{j}.pkl"
    output:
        RESULTS_DIR + "/{dataset}_annotations/09_catalytic/12_catalytic_residues_{j}.pkl"
    threads: 1
    resources:
        runtime=60,
        mem_mb=4000,
        nodes=1
    container:
        "docker://gzmf/get_catalytic:0.1"
    log:
        LOG_DIR + "/14_get_catalytic/{dataset}_{j}.log"
    shell:
        """
        python {SCRIPTS_DIR}/get_catalytic.py {input} {output} > {log} 2>&1
        """


def aggregate_output_catalytic(wildcards):
    checkpoint_output = checkpoints.get_uniprot_ids.get(**wildcards).output[0]
    return expand("{RESULTS_DIR}/{dataset}_annotations/09_catalytic/12_catalytic_residues_{j}.pkl",
                    RESULTS_DIR=RESULTS_DIR,
                    dataset=wildcards.dataset,
                    j=glob_wildcards(os.path.join(checkpoint_output, "07_variants_{j}.pkl")).j)


rule aggregate_catalytic:
    """
    Aggregate rule to check that the get_catalytic step was run correctly on all
    chunks.
    """
    input:
        aggregate_output_catalytic
    output:
        RESULTS_DIR + "/{dataset}_annotations/09_catalytic_done"
    threads: 1
    shell:
        "echo {input} > {output}"


rule combine_catalytic:
    """
    Combine all the pickle files with catalytic residues into one
    """
    input:
        expand("{RESULTS_DIR}/{dataset}_annotations/09_catalytic_done",
                RESULTS_DIR=RESULTS_DIR,
                dataset=config["datasets"])
    params:
        RESULTS_DIR + "/{dataset}_annotations/09_catalytic"
    output:
        RESULTS_DIR + "/{dataset}_annotations/13_catalytic_residues.pkl"
    threads: 1
    resources:
        runtime=120,
        mem_mb=16000,
        nodes=1
    container:
        "docker://gzmf/read_vcfs:0.1"
    log:
        LOG_DIR + "/15_combine_catalytic/{dataset}.log"
    shell:
        """
        python {SCRIPTS_DIR}/combine_catalytic.py {params} {output} > {log} 2>&1
        """

rule preprocess_variants:
    """
    Preprocess the variants and get DSSP file paths
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/08_variants/11_variants_{j}.pkl"
    output:
        RESULTS_DIR + "/{dataset}_annotations/10_preprocessed/14_variants_{j}.pkl"
    threads: 1
    resources:
        runtime=120,
        mem_mb=16000,
        nodes=1
    container:
        "docker://gzmf/calculate_features:0.1"
    log:
        LOG_DIR + "/16_preprocess_variants/{dataset}_{j}.log"
    shell:
        """
        python {SCRIPTS_DIR}/preprocess.py {input} \
        {DSSP_ALPHAFOLD} {DSSP_MANE} {output} > {log} 2>&1
        """

rule calculate_features:
    """
    Calculate the features for the variants
    """
    input:
        variants=RESULTS_DIR + "/{dataset}_annotations/10_preprocessed/14_variants_{j}.pkl",
        catalytic=RESULTS_DIR + "/{dataset}_annotations/13_catalytic_residues.pkl",
        pdbmodels=PDBMODELS,
        dssps=DSSPS
    output:
        RESULTS_DIR + "/{dataset}_annotations/11_features/15_variants_{j}.pkl"
    threads: 1
    resources:
        runtime=600,
        mem_mb=4000,
        nodes=1
    container:
        "docker://gzmf/calculate_features:0.1"
    log:
        LOG_DIR + "/17_calculate_features/{dataset}_{j}.log"
    shell:
        """
        python {SCRIPTS_DIR}/calculate_features.py {input.variants} \
        {input.catalytic} {PDBMODELS} {DSSPS} {output} > {log} 2>&1
        """

rule run_foldx:
    """
    Run FoldX on the variants
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/10_preprocessed/14_variants_{j}.pkl"
    output:
        RESULTS_DIR + "/{dataset}_annotations/12_foldx/17_foldx_energies_{j}.pkl"
    threads: 1
    resources:
        runtime=2880,
        mem_mb=4000,
        nodes=1
    container:
        "docker://gzmf/run_foldx:0.3"
    log:
        LOG_DIR + "/18_run_foldx/{dataset}_{j}.log"
    shell:
        """
        python {SCRIPTS_DIR}/run_foldx.py {input} {output} \
        --bin_dir resources/foldx --executable foldx_20251231 > {log} 2>&1
        """

rule write_for_dbnsfp:
    """
    Write the variants to a file to be used with dbNSFP
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/10_preprocessed/14_variants_{j}.pkl"
    output:
        RESULTS_DIR + "/{dataset}_annotations/13_dbnsfp/18_dbnsfp_variants_{j}.in"
    threads: 1
    resources:
        runtime=30,
        mem_mb=4000,
        nodes=1
    container:
        "docker://gzmf/read_vcfs:0.1"
    log:
        LOG_DIR + "/19_write_for_dbnsfp/{dataset}_{j}.log"
    shell:
        """
        python {SCRIPTS_DIR}/write_for_dbnsfp.py {input} {output} > {log} 2>&1
        """

rule run_dbnsfp:
    """
    Write the variants to a file to be used with dbNSFP
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/13_dbnsfp/18_dbnsfp_variants_{j}.in"
    output:
        fout=RESULTS_DIR + "/{dataset}_annotations/13_dbnsfp/18_dbnsfp_variants_{j}.out",
        ferr=RESULTS_DIR + "/{dataset}_annotations/13_dbnsfp/18_dbnsfp_variants_{j}.out.err"
    threads: 1
    resources:
        mem_mb=4096,
        runtime=120,
        nodes=1
    container:
        "docker://ibmjava:8-sfj"
    log:
        LOG_DIR + "/20_run_dbnsfp/{dataset}_{j}.log"
    shell:
        """
        java -Xmx4g search_dbNSFP49a -i {input} -o {output.fout} -w 1-6,12-18,190-211 > {log} 2>&1
        """


def aggregate_output_features(wildcards):
    checkpoint_output = checkpoints.get_uniprot_ids.get(**wildcards).output[0]
    return expand("{RESULTS_DIR}/{dataset}_annotations/11_features/15_variants_{j}.pkl",
                    RESULTS_DIR=RESULTS_DIR,
                    dataset=wildcards.dataset,
                    j=glob_wildcards(os.path.join(checkpoint_output, "07_variants_{j}.pkl")).j)

def aggregate_output_foldx(wildcards):
    checkpoint_output = checkpoints.get_uniprot_ids.get(**wildcards).output[0]
    return expand("{RESULTS_DIR}/{dataset}_annotations/12_foldx/17_foldx_energies_{j}.pkl",
                    RESULTS_DIR=RESULTS_DIR,
                    dataset=wildcards.dataset,
                    j=glob_wildcards(os.path.join(checkpoint_output, "07_variants_{j}.pkl")).j)

def aggregate_output_dbnsfp(wildcards):
    checkpoint_output = checkpoints.get_uniprot_ids.get(**wildcards).output[0]
    return expand("{RESULTS_DIR}/{dataset}_annotations/13_dbnsfp/18_dbnsfp_variants_{j}.out",
                    RESULTS_DIR=RESULTS_DIR,
                    dataset=wildcards.dataset,
                    j=glob_wildcards(os.path.join(checkpoint_output, "07_variants_{j}.pkl")).j)

rule aggregate_features:
    """
    Aggregate rule to check that the calculate_features and run_foldx steps were
    run correctly on all chunks.
    """
    input:
        aggregate_output_features,
        aggregate_output_foldx,
        aggregate_output_dbnsfp
    output:
        RESULTS_DIR + "/{dataset}_annotations/18_features_done"
    threads: 1
    shell:
        "echo {input} > {output}"


rule merge_variants:
    """
    Merge all the variants into a single DataFrame
    """
    input:
        expand("{RESULTS_DIR}/{dataset}_annotations/18_features_done",
                RESULTS_DIR=RESULTS_DIR,
                dataset=config["datasets"])
    params:
        features_dir=RESULTS_DIR + "/{dataset}_annotations/11_features",
        foldx_dir=RESULTS_DIR + "/{dataset}_annotations/12_foldx",
        dbnsfp_dir=RESULTS_DIR + "/{dataset}_annotations/13_dbnsfp"
    output:
        RESULTS_DIR + "/{dataset}_annotations/19_merged.pkl"
    threads: 40
    resources:
        runtime=600,
        mem_mb=1000000,
        nodes=1
    container:
        "docker://gzmf/read_vcfs:0.1"
    log:
        LOG_DIR + "/19_merge_variants/{dataset}.log"
    shell:
        """
        python {SCRIPTS_DIR}/merge_variants_optimized.py {params.features_dir} \
        {params.foldx_dir} {params.dbnsfp_dir} {output} --cpus {threads} > {log} 2>&1
        """

rule calculate_physicochemical:
    """
    Calculate the physicochemical properties of the variants
    """
    input:
        RESULTS_DIR + "/{dataset}_annotations/19_merged.pkl"
    output:
        RESULTS_DIR + "/{dataset}_annotations.pkl"
    threads: 1
    resources:
        runtime=120,
        mem_mb=128000,
        nodes=1
    container:
        "docker://gzmf/read_vcfs:0.1"
    log:
        LOG_DIR + "/20_physicochemical/{dataset}.log"
    shell:
        """
        python {SCRIPTS_DIR}/physicochemical.py {input} {PANTHER} {output} > {log} 2>&1
        """
